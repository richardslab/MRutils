---
output:
  pdf_document: default
  html_document: default
---

```{r}
library(tidyr)
library(plyr)
library(dplyr)
library(readxl)
library(curl)
library(jsonlite)
library(glue)

```

Data is organized in multiple columns with the same headers...need to clean this up and also figure out which of these tables is the "important" one.

```{r}
library(tidyr)
library(readxl)
data=read_excel("1-s2.0-S0002929720300173-mmc2.xlsx",skip = 4,.name_repair = "minimal")%>%subset(select=c(1:5,13:17))
first_blank=min(which(is.na(data$RSID)))
if(length(first_blank)==1){
  data=data[1:first_blank-1,]
}
names(data)[8]="beta"
data=data%>%mutate_at(.vars = c("EAF","MAF","beta","SE","P"),as.numeric)
significant_VD_snps=data%>%subset(P<5e-8 & MAF>=0.01)
write.table(significant_VD_snps,"significant_VD_SNPs.tsv",sep = "\t",row.names = F,quote=F)
```

Using tabix, extract the regions of interest from the gwas file:
```{r}
region=significant_VD_snps%>%alply(1,function(x)with(x, glue("{CHR}:{BP}-{BP}")))%>%(function(y) do.call(paste,y))
COVID="~/RichardsLab/VitaminD_MR/data/COVID19_HGI_A2_ALL_leave_23andme_20201020.b37.txt.gz"
system(glue("tabix -b 2 -c '#' -s 1 {COVID} {region} > response.txt"))
col_names=read.table(COVID,header=T,nrows = 1,comment.char = "")%>%names()
col_names[1]="CHR"
covid_data=read.table("response.txt",col.names = col_names)


```



Fill out the missing rsIDs (this can take a while if many are missing.)
```{r}

tee=function(x){
  print(x)
  x
}



getRsidFromPosition<-function(chrom,pos,ref,alt,assembly="hg19"){
  tryCatch({
    baseURL1="https://api.ncbi.nlm.nih.gov/variation/v0/vcf/{chrom}/{pos}/{ref}/{alt}/contextuals?assembly={assembly}"
    baseURL1_swapped="https://api.ncbi.nlm.nih.gov/variation/v0/vcf/{chrom}/{pos}/{alt}/{ref}/contextuals?assembly={assembly}"
    
    f=tryCatch({
      url=tee(glue(baseURL1))
      Sys.sleep(1)
      read_json(url)$data$spdis[[1]]
    },
    error=function(e){
      print("there was an error (1):")
      print(e)
      print("Trying swapping ref and alt")
      Sys.sleep(1)
      read_json(tee(glue(baseURL1_swapped)))$data$spdis[[1]]
    })
    
    pos=f$position
    seq_id=f$seq_id
    
    baseURL2="https://api.ncbi.nlm.nih.gov/variation/v0/spdi/{seq_id}:{pos}:{ref}:{alt}/rsids"
    baseURL2_swapped="https://api.ncbi.nlm.nih.gov/variation/v0/spdi/{seq_id}:{pos}:{alt}:{ref}/rsids"
    
    id=tryCatch({
      url=tee(glue(baseURL2))
      Sys.sleep(1)
      read_json(url)$data$rsids[[1]]
    },
    error=function(e){
      print("there was an error (2):")
      print(e)
      print("Trying swapping ref and alt")
      url=tee(glue(baseURL2_swapped))
      Sys.sleep(1)
      id=read_json(url)$data$rsids[[1]]
      glue("rs{id}")
    })
  }
  ,error=function(e) {
    print("there was an error:")
    print(e)
    NULL
  }
  )
}



`%notin%` <- Negate(`%in%`)


```

```{r, eval=FALSE}
unknown_ids=subset( covid_data,is.na(rsid),select = c(CHR,POS,REF,ALT))%>%transform(build="hg19")
#unknown_ids=subset( covid_data_with_rsids,is.na(rsid),select = c(CHR,POS,REF,ALT))%>%transform(build="hg19")

## this can take time and hits the API multuple times....
withIds=adply(unknown_ids, 1,function(x)c(rsid=getRsidFromPosition(chrom = x$CHR,pos = x$POS,ref = x$REF,alt=x$ALT,assembly = x$build)),.progress = "text")

write.table(withIds,"rsIdsFromdbSnp.txt",quote = F,sep = '\t',row.names = F)
```


```{r}
withIds=read.table("rsIdsFromdbSnp.txt",header=TRUE)

#withIds=mutate(withIds, rsid=RS_Number, RS_Number=NULL, build=NULL)

#merge the rsids


covid_data_with_rsids=mutate( merge(mutate(covid_data,rsid=as.character(rsid)), subset(withIds,select=c(CHR,POS,rsid)),all.x = T,by = c("CHR","POS")), rsid=adply(do.call(cbind,list(rsid.y,rsid.x)),1,function(x) first(na.omit(x)))$V1,rsid.x=NULL,rsid.y=NULL)


print(glue("there are {nrow(covid_data_with_rsids%>%subset(is.na(rsid)))} variants lacking rsid"))

merged=merge(significant_VD_snps,covid_data_with_rsids,by.x="RSID", by.y="rsid",all.x = T)

```




look for matches between sets
```{r eval=FALSE}
library(LDlinkR)
library(plyr)
library(reshape2)

token="151d285edb97" ## should be in a environment variable and input using 
pop=c("CEU","TSI","GBR","IBS")
# token = Sys.getenv("LDLINK_TOKEN")
# if you don't have a token go here: https://ldlink.nci.nih.gov/?tab=apiaccess

# this takes time and hits the LDlink API.
LDproxy_batch(subset(merged,is.na(CHR.y))$RSID,pop=pop,r2d = "r2",append = F,token = token)
```

```{r}
rsFiles=Filter(function(x)file.info(x)$size>1000,dir(".","^rs[0-9]*\\.txt"))

proxies = ldply(rsFiles, function(x) mutate(query_rsid=gsub("\\.txt$","",x),
                                            read.table(x,sep="\t")) %>% 
                  subset(R2 >= 0 & grepl("([ACGT]/[ACGT])", x=Alleles) )
)

proxies=proxies%>%subset(R2>=0.9)

```

Find proxies in covid data
```{r}

region=proxies%>%
  adply(1,function(x){split=strsplit(as.character(x$Coord),":")[[1]];data.frame(CHR=gsub("^chr","",split[[1]]),BP=split[[2]])})%>%
  alply(1,function(x)with(x, glue("{CHR}:{BP}-{BP}")))%>%(function(y) do.call(paste,y))
COVID="~/RichardsLab/VitaminD_MR/data/COVID19_HGI_A2_ALL_leave_23andme_20201020.b37.txt.gz"
system(tee(glue("tabix -b 2 -c '#' -s 1 {COVID} {region} > response2.txt")))
col_names=read.table(COVID,header=T,nrows = 1,comment.char = "")%>%names()
col_names[1]="CHR"
covid_data_proxies=read.table("response2.txt",col.names = col_names,stringsAsFactors = FALSE)
covid_data_proxies=merge(covid_data_proxies,proxies,by.x = "rsid", by.y="RS_Number")

chimeric=c("(A/T)", "(T/A)", "(C/G)", "(G/C)")
proxies_in_covid=merge(proxies,covid_data_proxies,by.x=c("RS_Number",intersect(names(proxies),names(covid_data_proxies))),by.y=c("rsid",intersect(names(proxies),names(covid_data_proxies))))%>%
  subset(Alleles%notin%chimeric)%>%
  group_by(query_rsid)%>%
  slice_max(order_by="all_inv_var_meta_p", n=1)

merged_with_proxies=merge(significant_VD_snps,select(proxies_in_covid,-MAF),by.x=c("RSID"),by.y=c("query_rsid"))%>%select(c(-"Distance",-"RS_Number",-"Coord",-"Alleles", -"Distance",-"Dprime",-"R2",-"Correlated_Alleles",-"RegulomeDB",-"Function"))%>%
rbind(merged,stringsAsFactors=FALSE)


```


Remove high LD snps

```{r}
library(LDlinkR)
library(plyr)
library(reshape2)

token="151d285edb97" ## should be in a environment variable and input using 
# token = Sys.getenv("LDLINK_TOKEN")
# if you don't have a token go here: https://ldlink.nci.nih.gov/?tab=apiaccess

LDPairs = ddply(merged_with_proxies, .variables = .(CHR.x), function(x) {
  if (nrow(x) > 1) {
    LDmatrix(x$RSID, pop = pop, token, r2d = "r2")%>%melt(id.vars="RS_number")
  }
})

LDpairs_culled=LDPairs

pairs=subset(LDpairs_culled,value>=0.05 & as.character(RS_number)!=as.character(variable))
while(nrow(pairs)>0){
  ##TODO: make sure to keep snps with smaller P value.....
  to_remove=pairs[which.max(pairs$value),"variable"]%>%as.character()
  LDpairs_culled=subset(LDpairs_culled,variable!=to_remove & RS_number!=to_remove)
  pairs=subset(LDpairs_culled,value>=0.05 & as.character(RS_number)!=as.character(variable))
}

merged_no_highLDs=subset(merged_with_proxies,RSID %in% (LDpairs_culled$RS_number%>%unique()))

print(glue("total number of SNPs for use in 2-Sample MR={nrow(merged_no_highLDs)}."))

write.table("SNPsFor2SMR.txt", merged_no_highLDs,sep = '\t',quote = F,row.names = F)

#TODO: figure out why there are duplicates in the SNP data...probably some merge...
```

Prepare snps with required data for use with 2sample MR
```{r}

library(TwoSampleMR)
exposure=read_exposure_data("SNPsFor2SMR.txt",sep = '\t',snp_col = "RSID",beta_col = "beta",se_col = "SE",eaf_col = "EAF",effect_allele_col = "EA",other_allele_col = "NEA",pval_col = "P")

outcome=read_outcome_data("SNPsFor2SMR.txt",sep='\t',snp_col="RSID",beta_col = "all_inv_var_meta_beta",se_col = "all_inv_var_meta_sebeta",eaf_col = "all_meta_AF",effect_allele_col = "EA",other_allele_col = "NEA",pval_col = "all_inv_var_meta_p",gene_col = "all_meta_N")
harmonized=harmonise_data(exposure,outcome)
results=mr(dat = harmonized)
single_snp_results=mr_singlesnp(harmonized)

show(mr_density_plot(single_snp_results,results))
show(plots)

show(mr_scatter_plot(results,harmonized))


```